{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a8fa9e",
   "metadata": {},
   "source": [
    "# 📌 1. What is Logistic Regression?\n",
    "\n",
    "**Logistic Regression** is a supervised machine learning algorithm used primarily for **binary classification** tasks. Unlike linear regression which predicts continuous values, logistic regression outputs a **probability** between 0 and 1 by applying the **sigmoid function** to a linear combination of input features.\n",
    "\n",
    "### 🔢 Key Concepts:\n",
    "- Uses **sigmoid/logistic function** to map outputs between 0 and 1.\n",
    "- The result is interpreted as the **probability of belonging to class 1**.\n",
    "- Often used for classification tasks like:\n",
    "  - Spam detection (spam vs. not spam)\n",
    "  - Fraud detection\n",
    "  - Disease prediction (yes/no)\n",
    "\n",
    "### 🧠 Logistic Function (Sigmoid):\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{where} \\quad z = w_0 + w_1x_1 + ... + w_nx_n\n",
    "$$\n",
    "If the result is > 0.5 → class 1, else → class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d95588",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1702c",
   "metadata": {},
   "source": [
    "# 📌 2. Real-world Use Cases\n",
    "\n",
    "Logistic Regression is popular in domains where **binary outcomes** are common and interpretability is important:\n",
    "\n",
    "### ✅ Popular Applications:\n",
    "- **Healthcare**: Predicting disease (e.g., diabetes, cancer risk)\n",
    "- **Finance**: Credit default prediction\n",
    "- **Marketing**: Customer churn prediction\n",
    "- **Cybersecurity**: Email spam detection\n",
    "- **HR**: Employee attrition prediction\n",
    "\n",
    "Its performance and ease of explanation to stakeholders make it a great baseline model in many industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc8654",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad411a1",
   "metadata": {},
   "source": [
    "# 📌 3. Math Behind Logistic Regression\n",
    "\n",
    "Logistic Regression uses a **linear combination of features** and passes it through the **sigmoid function** to get probabilities.\n",
    "\n",
    "### 🧮 Step-by-Step Intuition:\n",
    "1. Compute a linear score:\n",
    "$$\n",
    "z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "\n",
    "2. Convert score to probability:\n",
    "$$\n",
    "p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "3. Predict class:\n",
    "- If p > 0.5 → predict 1\n",
    "- If p ≤ 0.5 → predict 0\n",
    "\n",
    "4. Optimize parameters using **Log Loss**:\n",
    "$$\n",
    "\\text{LogLoss} = -\\frac{1}{n} \\sum (y_i \\log(p_i) + (1 - y_i)\\log(1 - p_i))\n",
    "$$\n",
    "\n",
    "Logistic Regression tries to **minimize Log Loss** using optimization techniques like **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6afc9",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e00b00",
   "metadata": {},
   "source": [
    "# 📌 4. Dataset Walkthroughs (Easy → Very Complex)\n",
    "\n",
    "Understanding how Logistic Regression behaves across datasets of different complexity is key to mastering it. Below is the dataset roadmap we’ll use:\n",
    "\n",
    "### 🟢 Easy Level\n",
    "- **Synthetic dataset** with 6 features (e.g., Glucose, BMI)\n",
    "- Balanced classes, clean data, no noise\n",
    "- Goal: Understand logistic regression mechanics\n",
    "\n",
    "### 🟡 Medium Level\n",
    "- Slightly imbalanced dataset (e.g., Titanic survival)\n",
    "- Categorical features requiring encoding\n",
    "- Missing values and basic preprocessing\n",
    "\n",
    "### 🟠 Complex Level\n",
    "- High-dimensional real-world data (e.g., Credit card fraud)\n",
    "- Requires scaling, regularization, class imbalance techniques\n",
    "- Involves feature selection or PCA\n",
    "\n",
    "### 🔴 Very Complex Level\n",
    "- Text classification or multi-class problems (e.g., Sentiment analysis)\n",
    "- Involves TF-IDF vectorization, NLP preprocessing\n",
    "- Requires robust evaluation with AUC, Log Loss, etc.\n",
    "\n",
    "---\n",
    "\n",
    "Each level will help us explore:\n",
    "- How logistic regression performs\n",
    "- Which preprocessing steps are necessary\n",
    "- How to interpret and tune performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10ac3a",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1018891",
   "metadata": {},
   "source": [
    "# 📌 5. Building Models at Each Level\n",
    "\n",
    "Now let’s walk through how to build Logistic Regression models for each dataset complexity — from beginner-friendly to real-world scale. Each level demonstrates different challenges and modeling strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟢 Easy Level: Synthetic Dataset\n",
    "- Dataset: Generated with `sklearn.datasets.make_classification`\n",
    "- Characteristics: Clean, small, balanced classes, no missing values\n",
    "- Goal: Focus purely on logistic regression behavior\n",
    "\n",
    "**Key Steps:**\n",
    "- No preprocessing required\n",
    "- Split data → Fit model → Evaluate accuracy, confusion matrix\n",
    "\n",
    "---\n",
    "\n",
    "### 🟡 Medium Level: Titanic Survival Dataset\n",
    "- Dataset: Classic binary classification problem\n",
    "- Characteristics: Mix of categorical and numerical data, some missing values\n",
    "- Goal: Learn preprocessing steps and model handling of imbalance\n",
    "\n",
    "**Key Steps:**\n",
    "- Handle missing values (e.g., fill age with median)\n",
    "- Encode categorical variables (e.g., `Sex`, `Embarked`)\n",
    "- Scale numerical features\n",
    "- Use `class_weight='balanced'` to deal with slight class imbalance\n",
    "\n",
    "---\n",
    "\n",
    "### 🟠 Complex Level: Credit Card Fraud Detection\n",
    "- Dataset: Highly imbalanced, noisy, anonymized features\n",
    "- Characteristics: ~280,000 rows, 1% fraud cases\n",
    "- Goal: Model performance under high class imbalance and data volume\n",
    "\n",
    "**Key Steps:**\n",
    "- Apply `StandardScaler` to features\n",
    "- Use undersampling / SMOTE for imbalance\n",
    "- Evaluate using AUC, F1 instead of accuracy\n",
    "- Tune regularization with `GridSearchCV`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔴 Very Complex Level: NLP/Text Sentiment Analysis\n",
    "- Dataset: IMDb reviews (binary sentiment)\n",
    "- Characteristics: Textual data → needs NLP transformation\n",
    "- Goal: Handle TF-IDF + dimensionality + multi-step preprocessing\n",
    "\n",
    "**Key Steps:**\n",
    "- Clean and tokenize text\n",
    "- Convert to numerical with TF-IDF\n",
    "- Use `LogisticRegression(solver='saga')` for sparse input\n",
    "- Perform hyperparameter tuning with `RandomizedSearchCV`\n",
    "- Evaluate with ROC-AUC and log loss\n",
    "\n",
    "---\n",
    "\n",
    "Each of these models will follow:\n",
    "> 🧪 Preprocessing → 🔧 Model Definition → 🚀 Training → 📊 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84a641",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea953c",
   "metadata": {},
   "source": [
    "# 📌 6. Hyperparameter Tuning Deep Dive\n",
    "\n",
    "Tuning hyperparameters can significantly improve the performance of your Logistic Regression model, especially as dataset complexity increases.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Why Tune Hyperparameters?\n",
    "\n",
    "Hyperparameters control **how** your model learns. In Logistic Regression, tuning helps with:\n",
    "- **Regularization strength**\n",
    "- **Solver optimization**\n",
    "- **Convergence speed**\n",
    "- **Model complexity control**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Key Hyperparameters to Tune\n",
    "\n",
    "| Hyperparameter | Purpose | Notes |\n",
    "|----------------|---------|-------|\n",
    "| `C` | Inverse of regularization strength | Smaller values specify stronger regularization |\n",
    "| `penalty` | Type of regularization to apply (`l1`, `l2`, `elasticnet`) | Depends on solver |\n",
    "| `solver` | Algorithm used in optimization (`liblinear`, `saga`, `newton-cg`) | `liblinear` works with small datasets, `saga` for large or sparse |\n",
    "| `max_iter` | Maximum number of iterations to converge | Increase if model doesn't converge |\n",
    "| `class_weight` | Handles class imbalance | `'balanced'` adjusts weights automatically |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Tuning Strategies by Dataset Level\n",
    "\n",
    "| Complexity | Tuning Method | Tools |\n",
    "|------------|----------------|-------|\n",
    "| Easy | Manual trial-and-error | Default parameters work well |\n",
    "| Medium | GridSearchCV | Explore combinations of 2-3 key params |\n",
    "| Complex | RandomizedSearchCV | Sample from hyperparameter space efficiently |\n",
    "| Very Complex | Optuna / Bayesian Optimization | Intelligent search based on performance history |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Example: Grid Search on Medium Dataset\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\\\"Best Params:\\\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774997c",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bad84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3916caf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_classification(n_samples=500,\n",
    "                           n_features=6,\n",
    "                           n_informative=4,\n",
    "                           n_redundant=0,\n",
    "                           class_sep=2.0,\n",
    "                           flip_y=0.01,\n",
    "                           random_state=42)\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# 4. Apply GridSearchCV\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 5. Best parameters\n",
    "print(\"✅ Best Parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc9235",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f2ff0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e1c508",
   "metadata": {},
   "source": [
    "# 📌 7. Evaluating a Model: Metrics to Monitor\n",
    "\n",
    "Evaluating a model correctly is just as important as building it — especially in classification tasks where accuracy alone can be misleading (especially on imbalanced datasets).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Core Evaluation Metrics for Logistic Regression\n",
    "\n",
    "| Metric | Goal | Use When | Formula/Explanation |\n",
    "|--------|------|----------|---------------------|\n",
    "| **Accuracy** | Higher is better | Balanced datasets | % of correctly predicted labels |\n",
    "| **Precision** | Higher is better | Cost of false positives is high | TP / (TP + FP) |\n",
    "| **Recall** | Higher is better | Cost of false negatives is high | TP / (TP + FN) |\n",
    "| **F1 Score** | Higher is better | Imbalanced datasets | Harmonic mean of Precision and Recall |\n",
    "| **AUC-ROC** | Higher is better | Probabilistic separability | Area under the ROC Curve |\n",
    "| **Log Loss** | Lower is better | Probabilistic predictions | Penalizes wrong confident predictions |\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 How to Use Them in Practice\n",
    "\n",
    "- **Confusion Matrix**: Helps break down TP, TN, FP, FN visually\n",
    "- **Classification Report**: Combines all important scores\n",
    "- **ROC Curve**: Visualize model's ability to distinguish classes\n",
    "- **Threshold Tuning**: Change cutoff probability from 0.5 to optimize F1 or recall\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Sample Code to Evaluate Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52845ecb",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632d20bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80        49\n",
      "           1       0.83      0.75      0.78        51\n",
      "\n",
      "    accuracy                           0.79       100\n",
      "   macro avg       0.79      0.79      0.79       100\n",
      "weighted avg       0.79      0.79      0.79       100\n",
      "\n",
      "🧾 Confusion Matrix:\n",
      "[[41  8]\n",
      " [13 38]]\n",
      "n📈 ROC AUC Score: 0.8907563025210085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Predict\n",
    "y_pred = grid.predict(X_test)\n",
    "y_proba = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"🧾 Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# AUC Score\n",
    "print(\"n📈 ROC AUC Score:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297fa411",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be391d68",
   "metadata": {},
   "source": [
    "### 🔍 Interpretation:\n",
    "- The model performs **fairly balanced** across both classes.\n",
    "- Slight trade-off between **precision and recall**.\n",
    "- **AUC of 0.89** indicates strong class separability — excellent for an easy-level dataset!\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 Additional Learning Resources\n",
    "\n",
    "- [sklearn LogisticRegression Docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Visual Introduction to Logistic Regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)\n",
    "- [Confusion Matrix Explained](https://ml-cheatsheet.readthedocs.io/en/latest/classification.html)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 GitHub Repo & Medium Article\n",
    "\n",
    "Once you’ve completed all complexity levels and polishing:\n",
    "\n",
    "📁 GitHub:  \n",
    "➡️ `https://github.com/yourusername/logistic-regression-learning`\n",
    "\n",
    "✍️ Medium:  \n",
    "➡️ `https://medium.com/@yourname/logistic-regression-explained`\n",
    "\n",
    "---\n",
    "\n",
    "### 🏁 Final Note\n",
    "\n",
    "Logistic Regression is a powerful, interpretable baseline model. Mastering it sets the foundation for exploring more complex ML and deep learning techniques 🚀\n",
    "\n",
    "Let’s now build out the **medium-level model**, or would you prefer to export this `.ipynb` file first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c13ab1",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab73d40",
   "metadata": {},
   "source": [
    "# 📌 8. Conclusion + Resources + GitHub Link\n",
    "\n",
    "### 🎯 Summary\n",
    "In this notebook, we’ve walked through Logistic Regression from theory to practice — covering intuition, real-world use cases, dataset walkthroughs, model building across complexity levels, hyperparameter tuning, and model evaluation.\n",
    "\n",
    "### 🧪 Final Model Evaluation (on Easy Dataset)\n",
    "**Classification Report:**\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| 0     | 0.76      | 0.84   | 0.80     | 49      |\n",
    "| 1     | 0.83      | 0.75   | 0.78     | 51      |\n",
    "\n",
    "- **Accuracy**: 0.79\n",
    "- **Macro Avg F1**: 0.79\n",
    "- **ROC AUC Score**: **0.89** 🔥\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "Predicted\n",
    "      0     1\n",
    "    ----------\n",
    "0 |  41   |  8\n",
    "1 |  13   | 38\n",
    "```\n",
    "\n",
    "### 🔍 Interpretation\n",
    "- The model shows balanced precision and recall\n",
    "- AUC of 0.89 shows great class separability\n",
    "- Slightly higher recall on class 0, better precision on class 1\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 Additional Learning Resources\n",
    "- [Scikit-Learn Docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Towards Data Science Logistic Regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)\n",
    "- [Confusion Matrix Guide](https://ml-cheatsheet.readthedocs.io/en/latest/classification.html)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 GitHub & Medium Links\n",
    "- GitHub: `https://github.com/anirudhyadav/Logistic-Regression`\n",
    "- Medium: `https://medium.com/@yourname/logistic-regression-explained`\n",
    "\n",
    "---\n",
    "Thank you for following along 🙌. Logistic Regression is a powerful first step in your machine learning journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7564fd6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
