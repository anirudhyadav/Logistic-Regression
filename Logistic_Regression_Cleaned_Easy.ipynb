{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a8fa9e",
   "metadata": {},
   "source": [
    "# üìå 1. What is Logistic Regression?\n",
    "\n",
    "**Logistic Regression** is a supervised machine learning algorithm used primarily for **binary classification** tasks. Unlike linear regression which predicts continuous values, logistic regression outputs a **probability** between 0 and 1 by applying the **sigmoid function** to a linear combination of input features.\n",
    "\n",
    "### üî¢ Key Concepts:\n",
    "- Uses **sigmoid/logistic function** to map outputs between 0 and 1.\n",
    "- The result is interpreted as the **probability of belonging to class 1**.\n",
    "- Often used for classification tasks like:\n",
    "  - Spam detection (spam vs. not spam)\n",
    "  - Fraud detection\n",
    "  - Disease prediction (yes/no)\n",
    "\n",
    "### üß† Logistic Function (Sigmoid):\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{where} \\quad z = w_0 + w_1x_1 + ... + w_nx_n\n",
    "$$\n",
    "If the result is > 0.5 ‚Üí class 1, else ‚Üí class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d95588",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1702c",
   "metadata": {},
   "source": [
    "# üìå 2. Real-world Use Cases\n",
    "\n",
    "Logistic Regression is popular in domains where **binary outcomes** are common and interpretability is important:\n",
    "\n",
    "### ‚úÖ Popular Applications:\n",
    "- **Healthcare**: Predicting disease (e.g., diabetes, cancer risk)\n",
    "- **Finance**: Credit default prediction\n",
    "- **Marketing**: Customer churn prediction\n",
    "- **Cybersecurity**: Email spam detection\n",
    "- **HR**: Employee attrition prediction\n",
    "\n",
    "Its performance and ease of explanation to stakeholders make it a great baseline model in many industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc8654",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad411a1",
   "metadata": {},
   "source": [
    "# üìå 3. Math Behind Logistic Regression\n",
    "\n",
    "Logistic Regression uses a **linear combination of features** and passes it through the **sigmoid function** to get probabilities.\n",
    "\n",
    "### üßÆ Step-by-Step Intuition:\n",
    "1. Compute a linear score:\n",
    "$$\n",
    "z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "\n",
    "2. Convert score to probability:\n",
    "$$\n",
    "p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "3. Predict class:\n",
    "- If p > 0.5 ‚Üí predict 1\n",
    "- If p ‚â§ 0.5 ‚Üí predict 0\n",
    "\n",
    "4. Optimize parameters using **Log Loss**:\n",
    "$$\n",
    "\\text{LogLoss} = -\\frac{1}{n} \\sum (y_i \\log(p_i) + (1 - y_i)\\log(1 - p_i))\n",
    "$$\n",
    "\n",
    "Logistic Regression tries to **minimize Log Loss** using optimization techniques like **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6afc9",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e00b00",
   "metadata": {},
   "source": [
    "# üìå 4. Dataset Walkthroughs (Easy ‚Üí Very Complex)\n",
    "\n",
    "Understanding how Logistic Regression behaves across datasets of different complexity is key to mastering it. Below is the dataset roadmap we‚Äôll use:\n",
    "\n",
    "### üü¢ Easy Level\n",
    "- **Synthetic dataset** with 6 features (e.g., Glucose, BMI)\n",
    "- Balanced classes, clean data, no noise\n",
    "- Goal: Understand logistic regression mechanics\n",
    "\n",
    "### üü° Medium Level\n",
    "- Slightly imbalanced dataset (e.g., Titanic survival)\n",
    "- Categorical features requiring encoding\n",
    "- Missing values and basic preprocessing\n",
    "\n",
    "### üü† Complex Level\n",
    "- High-dimensional real-world data (e.g., Credit card fraud)\n",
    "- Requires scaling, regularization, class imbalance techniques\n",
    "- Involves feature selection or PCA\n",
    "\n",
    "### üî¥ Very Complex Level\n",
    "- Text classification or multi-class problems (e.g., Sentiment analysis)\n",
    "- Involves TF-IDF vectorization, NLP preprocessing\n",
    "- Requires robust evaluation with AUC, Log Loss, etc.\n",
    "\n",
    "---\n",
    "\n",
    "Each level will help us explore:\n",
    "- How logistic regression performs\n",
    "- Which preprocessing steps are necessary\n",
    "- How to interpret and tune performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10ac3a",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1018891",
   "metadata": {},
   "source": [
    "# üìå 5. Building Models at Each Level\n",
    "\n",
    "Now let‚Äôs walk through how to build Logistic Regression models for each dataset complexity ‚Äî from beginner-friendly to real-world scale. Each level demonstrates different challenges and modeling strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Easy Level: Synthetic Dataset\n",
    "- Dataset: Generated with `sklearn.datasets.make_classification`\n",
    "- Characteristics: Clean, small, balanced classes, no missing values\n",
    "- Goal: Focus purely on logistic regression behavior\n",
    "\n",
    "**Key Steps:**\n",
    "- No preprocessing required\n",
    "- Split data ‚Üí Fit model ‚Üí Evaluate accuracy, confusion matrix\n",
    "\n",
    "---\n",
    "\n",
    "### üü° Medium Level: Titanic Survival Dataset\n",
    "- Dataset: Classic binary classification problem\n",
    "- Characteristics: Mix of categorical and numerical data, some missing values\n",
    "- Goal: Learn preprocessing steps and model handling of imbalance\n",
    "\n",
    "**Key Steps:**\n",
    "- Handle missing values (e.g., fill age with median)\n",
    "- Encode categorical variables (e.g., `Sex`, `Embarked`)\n",
    "- Scale numerical features\n",
    "- Use `class_weight='balanced'` to deal with slight class imbalance\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Complex Level: Credit Card Fraud Detection\n",
    "- Dataset: Highly imbalanced, noisy, anonymized features\n",
    "- Characteristics: ~280,000 rows, 1% fraud cases\n",
    "- Goal: Model performance under high class imbalance and data volume\n",
    "\n",
    "**Key Steps:**\n",
    "- Apply `StandardScaler` to features\n",
    "- Use undersampling / SMOTE for imbalance\n",
    "- Evaluate using AUC, F1 instead of accuracy\n",
    "- Tune regularization with `GridSearchCV`\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Very Complex Level: NLP/Text Sentiment Analysis\n",
    "- Dataset: IMDb reviews (binary sentiment)\n",
    "- Characteristics: Textual data ‚Üí needs NLP transformation\n",
    "- Goal: Handle TF-IDF + dimensionality + multi-step preprocessing\n",
    "\n",
    "**Key Steps:**\n",
    "- Clean and tokenize text\n",
    "- Convert to numerical with TF-IDF\n",
    "- Use `LogisticRegression(solver='saga')` for sparse input\n",
    "- Perform hyperparameter tuning with `RandomizedSearchCV`\n",
    "- Evaluate with ROC-AUC and log loss\n",
    "\n",
    "---\n",
    "\n",
    "Each of these models will follow:\n",
    "> üß™ Preprocessing ‚Üí üîß Model Definition ‚Üí üöÄ Training ‚Üí üìä Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84a641",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea953c",
   "metadata": {},
   "source": [
    "# üìå 6. Hyperparameter Tuning Deep Dive\n",
    "\n",
    "Tuning hyperparameters can significantly improve the performance of your Logistic Regression model, especially as dataset complexity increases.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why Tune Hyperparameters?\n",
    "\n",
    "Hyperparameters control **how** your model learns. In Logistic Regression, tuning helps with:\n",
    "- **Regularization strength**\n",
    "- **Solver optimization**\n",
    "- **Convergence speed**\n",
    "- **Model complexity control**\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Key Hyperparameters to Tune\n",
    "\n",
    "| Hyperparameter | Purpose | Notes |\n",
    "|----------------|---------|-------|\n",
    "| `C` | Inverse of regularization strength | Smaller values specify stronger regularization |\n",
    "| `penalty` | Type of regularization to apply (`l1`, `l2`, `elasticnet`) | Depends on solver |\n",
    "| `solver` | Algorithm used in optimization (`liblinear`, `saga`, `newton-cg`) | `liblinear` works with small datasets, `saga` for large or sparse |\n",
    "| `max_iter` | Maximum number of iterations to converge | Increase if model doesn't converge |\n",
    "| `class_weight` | Handles class imbalance | `'balanced'` adjusts weights automatically |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Tuning Strategies by Dataset Level\n",
    "\n",
    "| Complexity | Tuning Method | Tools |\n",
    "|------------|----------------|-------|\n",
    "| Easy | Manual trial-and-error | Default parameters work well |\n",
    "| Medium | GridSearchCV | Explore combinations of 2-3 key params |\n",
    "| Complex | RandomizedSearchCV | Sample from hyperparameter space efficiently |\n",
    "| Very Complex | Optuna / Bayesian Optimization | Intelligent search based on performance history |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Example: Grid Search on Medium Dataset\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\\\"Best Params:\\\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774997c",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bad84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3916caf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_classification(n_samples=500,\n",
    "                           n_features=6,\n",
    "                           n_informative=4,\n",
    "                           n_redundant=0,\n",
    "                           class_sep=2.0,\n",
    "                           flip_y=0.01,\n",
    "                           random_state=42)\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# 4. Apply GridSearchCV\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 5. Best parameters\n",
    "print(\"‚úÖ Best Parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc9235",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f2ff0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e1c508",
   "metadata": {},
   "source": [
    "# üìå 7. Evaluating a Model: Metrics to Monitor\n",
    "\n",
    "Evaluating a model correctly is just as important as building it ‚Äî especially in classification tasks where accuracy alone can be misleading (especially on imbalanced datasets).\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Core Evaluation Metrics for Logistic Regression\n",
    "\n",
    "| Metric | Goal | Use When | Formula/Explanation |\n",
    "|--------|------|----------|---------------------|\n",
    "| **Accuracy** | Higher is better | Balanced datasets | % of correctly predicted labels |\n",
    "| **Precision** | Higher is better | Cost of false positives is high | TP / (TP + FP) |\n",
    "| **Recall** | Higher is better | Cost of false negatives is high | TP / (TP + FN) |\n",
    "| **F1 Score** | Higher is better | Imbalanced datasets | Harmonic mean of Precision and Recall |\n",
    "| **AUC-ROC** | Higher is better | Probabilistic separability | Area under the ROC Curve |\n",
    "| **Log Loss** | Lower is better | Probabilistic predictions | Penalizes wrong confident predictions |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà How to Use Them in Practice\n",
    "\n",
    "- **Confusion Matrix**: Helps break down TP, TN, FP, FN visually\n",
    "- **Classification Report**: Combines all important scores\n",
    "- **ROC Curve**: Visualize model's ability to distinguish classes\n",
    "- **Threshold Tuning**: Change cutoff probability from 0.5 to optimize F1 or recall\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Sample Code to Evaluate Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52845ecb",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632d20bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80        49\n",
      "           1       0.83      0.75      0.78        51\n",
      "\n",
      "    accuracy                           0.79       100\n",
      "   macro avg       0.79      0.79      0.79       100\n",
      "weighted avg       0.79      0.79      0.79       100\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      "[[41  8]\n",
      " [13 38]]\n",
      "nüìà ROC AUC Score: 0.8907563025210085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Predict\n",
    "y_pred = grid.predict(X_test)\n",
    "y_proba = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"üßæ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# AUC Score\n",
    "print(\"nüìà ROC AUC Score:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297fa411",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be391d68",
   "metadata": {},
   "source": [
    "### üîç Interpretation:\n",
    "- The model performs **fairly balanced** across both classes.\n",
    "- Slight trade-off between **precision and recall**.\n",
    "- **AUC of 0.89** indicates strong class separability ‚Äî excellent for an easy-level dataset!\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Learning Resources\n",
    "\n",
    "- [sklearn LogisticRegression Docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Visual Introduction to Logistic Regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)\n",
    "- [Confusion Matrix Explained](https://ml-cheatsheet.readthedocs.io/en/latest/classification.html)\n",
    "\n",
    "---\n",
    "\n",
    "### üîó GitHub Repo & Medium Article\n",
    "\n",
    "Once you‚Äôve completed all complexity levels and polishing:\n",
    "\n",
    "üìÅ GitHub:  \n",
    "‚û°Ô∏è `https://github.com/yourusername/logistic-regression-learning`\n",
    "\n",
    "‚úçÔ∏è Medium:  \n",
    "‚û°Ô∏è `https://medium.com/@yourname/logistic-regression-explained`\n",
    "\n",
    "---\n",
    "\n",
    "### üèÅ Final Note\n",
    "\n",
    "Logistic Regression is a powerful, interpretable baseline model. Mastering it sets the foundation for exploring more complex ML and deep learning techniques üöÄ\n",
    "\n",
    "Let‚Äôs now build out the **medium-level model**, or would you prefer to export this `.ipynb` file first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c13ab1",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab73d40",
   "metadata": {},
   "source": [
    "# üìå 8. Conclusion + Resources + GitHub Link\n",
    "\n",
    "### üéØ Summary\n",
    "In this notebook, we‚Äôve walked through Logistic Regression from theory to practice ‚Äî covering intuition, real-world use cases, dataset walkthroughs, model building across complexity levels, hyperparameter tuning, and model evaluation.\n",
    "\n",
    "### üß™ Final Model Evaluation (on Easy Dataset)\n",
    "**Classification Report:**\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| 0     | 0.76      | 0.84   | 0.80     | 49      |\n",
    "| 1     | 0.83      | 0.75   | 0.78     | 51      |\n",
    "\n",
    "- **Accuracy**: 0.79\n",
    "- **Macro Avg F1**: 0.79\n",
    "- **ROC AUC Score**: **0.89** üî•\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "Predicted\n",
    "      0     1\n",
    "    ----------\n",
    "0 |  41   |  8\n",
    "1 |  13   | 38\n",
    "```\n",
    "\n",
    "### üîç Interpretation\n",
    "- The model shows balanced precision and recall\n",
    "- AUC of 0.89 shows great class separability\n",
    "- Slightly higher recall on class 0, better precision on class 1\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Learning Resources\n",
    "- [Scikit-Learn Docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Towards Data Science Logistic Regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)\n",
    "- [Confusion Matrix Guide](https://ml-cheatsheet.readthedocs.io/en/latest/classification.html)\n",
    "\n",
    "---\n",
    "\n",
    "### üîó GitHub & Medium Links\n",
    "- GitHub: `https://github.com/anirudhyadav/Logistic-Regression`\n",
    "- Medium: `https://medium.com/@yourname/logistic-regression-explained`\n",
    "\n",
    "---\n",
    "Thank you for following along üôå. Logistic Regression is a powerful first step in your machine learning journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7564fd6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
